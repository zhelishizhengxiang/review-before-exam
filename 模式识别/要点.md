## 第一章 绪论
1.模式识别的定义：从工程的角度研究如何使计算机具有识别能力的理论和方法
2.模式：识别对象的一组属性集合（比如{颜色、形状、外表、酸度}）
3.识别：根据模式判断不同的对象是否属于同类或者属于某个类别
4.模式识别与其他相似概念的范围从大到小为：人工智能->机器学习->模式识别->神经网络->深度学习
5.模式识别系统完成的工作包含以下步骤（即数据的采集、分析和处理包含以下步骤）
    1）数据采集及预处理
    2）特征生成
    3）特征提取与选择
    4）识别分类
6.数据采集：将需要识别的对象数字化为波形、图像、文字等计算机可以处理的形式
7.预处理：将数据中混杂的与识别对象无关的噪声去除
8.特征：描述对象之间差异的属性（比如{颜色、形状、外表、酸度}）
9.特征提取与选择，生成尽可能多的特征，从中选择或组合出最有效的特征
10.识别分类：根据特征矢量判断样本类别，该过程是从特征空间像类别空间的一个映射
![alt text](assets/a256ebbbec19b8a43b5f75a650dd3b2.jpg)
11.分类器一般需要一个训练和学习的过程
12.完整的模式识别系统包括**训练和识别**两个过程
![alt text](assets/043952093152faf8f40f3fe83c2f6f9.jpg)
13.特征提取会生成新特征，特征选择不会生成新特征
14.本门课的主要内容：让计算机如何自动设计分类器，即介绍各种识别分类方法
15.数据样本集的划分：
1）训练集：用于训练模型的参数
2）验证集：模型的一些超参数（比如k-means中的k，要分成几簇中的k）
3）测试集：评价模型的最终性能。模型的性能最是在测试集上进行测试，而不是在训练集或验证集

16.模式识别方法可分为有监督学习和无监督学习。有监督学习（分类）：预先已知训练样本和训练样本的所属类别；无监督学习（聚类）：预先不知道训练样本的类别，也不知道类别数量
17.有监督学习又可以分为鉴判别式模型和产生式模型。判别式模型：样本模式**确定地处于特征空间不同区域**，通过训练得到类别边界；产生式模型：样本模式**是特征空间的随机变量**，估计概率密度确定类别属性
18.只有基于贝叶斯理论的是产生式模型，其他的都是鉴别模型。


## 第二章距离分类器
1.距离分类器的一般形式：待识别样本x分类到与其最相似（特征空间内距离最接近）的类别中
2.度量样本与类别的相似度（距离）的方法：模板匹配、最近邻分类、K-近邻算法
3.模板匹配：选出每个类别的模板，即代表样本ui，用待识别样本x和ui的距离来衡量相似度，距离越大，相似度越低
4.每个类别的区域：
![alt text](assets/c8de05d0aaaa3b3e23d464b7e6eda74.jpg)
5.最近邻分类相似度的刻画：样本x与类别w的相似度可以用x与该类别中的**最近**样本的距离来度量。
6.最近邻分类的方法和输出：在所有类别的训练样本集合D中寻找与X**最近的1个**样本y，y所属的类别就是待识别样本的类别。
7.![alt text](assets/93c62b09ff6f00bb0e654bd85bb7b0c.jpg)
8.最近邻分类的缺点：
1）计算量大（与所有训练样本计算距离获得最小）
2）占用存储空间大（保存所有训练样本）
3）易受样本噪声影响（离群点）
9.K近邻算法（消除最近邻的易受噪声影响的缺陷）的方法与输出：在所有类别的训练样本集合D中寻找与X**最近的K个**样本，统计其中属于各个类别的样本数，选择样本数最高的类别作为待识别样本x的类别
10.最近邻算法是K-近邻的特例
11.当K值选择过小，算法性能接近于最近邻分类，K值选择过大，距离较远的样本也会对分类结果产生影响
12.欧氏距离：特征空间两点间距离（传统意义上的距离）
13.曼哈顿距离：特征距离中两点坐标差的绝对值之和
14.切比雪夫距离：个坐标数值差绝对值的最大值
15.泛化性能：分类器识别未知标签的测试数据类别的能力
16:训练性能：分类器识别训练数据类别的能力
17.模式识别的任务（什么是好的分类器）：能够**提供最佳泛化性能**的分类器，而非提供最佳训练性能的分类器
19.过拟合（也叫过学习）：分类器过于复杂，学习到噪声或者随机误差
20.欠拟合：分类器过于简单。
21.最佳分类器：应该在**测试数据**上表现最好
![alt text](assets/484a1977880b61ca143f7a2b853bbb8.jpg)
22.分类器的评价指标有如下：
1）准确率/错误率（准确率不适用于不平衡数据集，准确率无法反应分类错误的代价）
2）拒识率：只对有把握的样本判别类别，对没有把握的样本拒绝识别。eg：共m个样本，mr个被拒识，mc个被准确分类，me个被错误分类，则拒识率为mr/m，准确率mc/m-mr
![alt text](assets/ac75fd9f46417cf64a2614841755478.jpg)
3）![alt text](assets/b32b6cf3514212404e5b281a976fc29.jpg)

23.评价方法矩阵如下：
1）两分法：随机将样本集划分为两个不相交的子集，分别用于训练和测试。划分可以随机重复若干次，结果取均值
2）交叉验证：具体如下。
![alt text](assets/60255e6f00729d241cad49c7d4bd45c.jpg)
3）bootstrap方法


## 第四章 线性判别函数分类器（知道概念，不必深究）
1.线性分类界面：直线、平面和超平面都可以称为线性分类界面
2.采用线性分类界面区分两类样本的方法称为线性分类器
3.线性判别分类器函数的形式（只需要知道下面的这个即可）：g(x)>0,属于一个类别，小于0属于另一个类别，=0拒识
4.最小平方误差（也叫最小均方误差）算法的思想：假如说有两个样本，确实是不可分，就希望分的误差越小越好
5.最小平方误差算法的特点：
1）对于线性不可分的训练样本，算法能够收敛于一个均方误差最小解
2）对于线性可分的训练样本，算法未必能收敛于一个分类超平面

6.线性判别函数分类器同人工神经元有密切联系

##  第六章 非线性判别函数分类器
1.解决非线性问题的两个方法：多层感知器网络、支持向量机
2.升维（特征矢量）是把线性不可分转化为线性可分的情况的的重要手段（对于异或问题，从二维增加一个维度{x1，x2}->{x1，x2，x1x2}）
3.广义线性判别函数分类器：通过的增加定义于原空间上的非线性函数作为特征，从而提升线性可分能力

## 第五章 特征选择和特征提取
1.**两类降维方法：特征选择和特征提取**
2.更多的特征并不意味着更好的分类结果（维数诅咒）
3.分类器的学习是利用训练样本估计参数的过程（样本数越多，参数估计越准确；样本数一定的情况下，参数数量越少则估计越准确；少量样本估计过多参数是不可靠的）
4.稀疏化的样本难以正确估计模型参数，容易过拟合
5.特征选择：从原始生成的d维特征中选取d'个特征构成新的特征矢量的过程，其中d'<d。即从原始特征中挑选出最有价值的一组特征子集，
6.特征选择并不生成新特征。
7.特征提取：根据原始d维特征的内在信息，通过某种函数变换y=f(x)重新生成一直新的d'维特征，d'<d。
8.我们使用类别可分性判据作为度量特征分类价值（即此特征对于这两个类别的分类贡献的价值大小）的指标
9.两种常用的类别可分性判据：1）基于距离的 2）基于散布矩阵
10.基于距离的类别可分性判据：
1）类内距离：度量特征集X上同类样本的相似程度
2）类间距离：度量不同类别样本之间的差异程度
11.类别可分性判据的原则：
1）可分性判据越大越好（越可分）
2）最小化类内距离、最大化类间距离

12.**特征选择的3个方法：1）过滤法 2）包装法 3）嵌入法**
13.过滤法：按照类别可分性判据等指标评价**每个特征**，根据判据值从大到小排序，挑选判据值最大的前d'个特征。（缺点：如果特征之间存在相关性，此方法不保证最优性；只有特征相互独立时，才可以保证最优性）
14.包装法：将类别可分性判据作为**每个特征子集**的评价标准，比较多种可能子集的性能
15.嵌入法：将特征选择与分类器训练融为一体，在分类器训练过程中自动选择特征
16.**特征提取的两个方法（变换y=f(x)是线性函数时）：1）主成分分析 2）基于fisher准则的可分性分析**
17.主成分分析（也称为主分量分析）（principal component analysis）的主要思想：寻找到数据的主轴方向（是数据方差最大的投影方向），由主轴构成一个新的坐标系（维数比原维数低），然后数据由原坐标系向新坐标系投影
18.PCA从尽可能减少信息损失的角度实现降维
19.PCA算法步骤：
1）根据样本集合D计算均值矢量和协方差矩阵
2）计算特征值和特征矢量，按照特征值由大到小排序
3）选择前d'个特征矢量作为列矢量构成矩阵


20.**PCA要点**：
1）**特征矢量正交**：主成分分析得到的新坐标系是直角坐标系
2）**变换后特征不相关：主成分分析可以消除特征间的相关性**
3）**特征值即信息量**：特征值越大，正在对应特征向量上的方差越大，样本点越分散，越容易区分，信息量就越多

21.冗余特征：特征值为0或接近于0的特征为冗余特征
22.主成分分析用的d'个新特征表示原来d维特征是有误差的，特征值越小，误差越小。
23.**PCA属于无监督学习**
24.第一主成分（第一投影方向）：是数据方差最大的投影方向，第二主成分是数据方差次大的投影方向

25.基于fisher准则的可分性分析（又称为线性判别分析、线性鉴别分析LDA）（fisher discriminant analysis）：目标是找到一个投影方向，将原始维空间中的样本投影到一维空间中，使得投影后**两类样本的均值之差尽可能大，同时类内方差尽可能小**，从而实现两类样本的有效分离
26.Fisher推导出来的结果：两个类别样本向一条直线上投影，直线方向为矩阵S_w^(-1) * S_b最大特征值对应的特征矢量时，可以使得投影具有最大可分性
27.FDA算法步骤：
1）计算类内散布矩阵S_w和类间散布矩阵S_b
2）计算矩阵S_w^(-1) * S_b的特征值和特征矢量，按特征值由大到小排序
3）选择前d'个特征矢量作为列矢量构成矩阵

28.**FDA的要点：**
1）**特征矢量不具有正交性
2）变换后特征仍具有相关性
3）对于c个类别的样本集，矩阵S_w^(-1) * S_b至多存在c-1个特征值大于等于0，其他的d-c+1特征值均为0
4）由第三条可以知道，FDA后新坐标维数最多为c-1**

29.**FDA属于有监督学习**
30：FDA第一投影方向：类别差异性最大的方向
30.PCA和FDA的的对比具体如下图所示
![alt text](assets/c1b472d3756e6fe950cd1b74a7f920d.jpg)

## 第七章 统计分类器及其学习
1.产生式模型的基础是贝叶斯决策理论
2.贝叶斯公式中的一些参数表示，如下图所示
![alt text](assets/2415c728ef594f46978d24b8a7f8b32.jpg)
![alt text](assets/76fac76075ad53cccc98f536cfd2e89.jpg)
![alt text](assets/a3142981d345299fee3bd686850ba1a.jpg)
3.分类器的分类依据是后验概率，而不是先验概率
4.贝叶斯分类的基础是贝叶斯公式
5.贝叶斯公式：具体如下图所示（判别属于哪一种情况可以不算p(x)，具体需要求出后验概率时需要使用全概率公式求出p(x)）
![alt text](assets/4af96177af58c042dc1644559ea943d.jpg)
6.先验概率的改变会引起后验概率的改变
7.贝叶斯分类决策具有错误率
8.贝叶斯分类决策的方法：1）基于最小错误率 2）基于最小平均风险
9.基于最小错误率的贝叶斯决策：算出每个类别的后验概率，将x判别为后验概率最大的一个类别。
10.基于最小平均风险的贝叶斯决策：引入风险值评估。具体如下图
![alt text](assets/b6b0b58e80a402b1015c966df8ea2de.jpg)
![alt text](assets/3e0092ad16cc51aed5d62eecbf785c6.jpg)
![alt text](assets/0162fb59b31f31515c76657624d3bd0.jpg)
11.当风险值时0-1损失函数时，最小平均风险贝叶斯决策就等价于最小错误率贝叶斯决策

12.解决多特征少样本贝叶斯分类的一种方法采用朴素贝叶斯分类
13.类别先验概率P(w)的估计相对容易（依靠经验），类条件概率密度p(x|w)的估计相对较难
14.![alt text](assets/0166895c9376c76c3ef2427cd02f438.jpg)
15.参数估计分为非监督参数估计和监督参数估计
16.非参数估计都是有监督学习
17.![alt text](assets/fde6c5d49271d884672bf4a4bd6ee37.jpg)
![alt text](assets/923be2077389478392ab4afbfc87a77.jpg)
18.**概率密度函数的非参数估计的两种办法：Parzen窗方法和近邻法**
19.**概率密度函数的参数估计的两种办法：最大似然估计和贝叶斯估计**
![alt text](assets/938a02d66cb0d0c2b18e2ef74568ad1.jpg)
20.parzen窗法只需要知道以下内容：
1）存在的主要困难：窗宽选择
2）宽度过小，得到的密度曲线抖动过大；宽度过大，得到的曲线过于平滑
3）优点：普遍适应
![alt text](assets/8ee6595652bde4d53ef3574972df917.jpg)
21.![alt text](assets/bd9ab6220b6410a13425cfc097c35fa.jpg)
22.近邻法具体算法跟k近邻一模一样。
![alt text](assets/bb732e9563b7c92ca23760d8e814f85.jpg)
23.最大似然估计的内容如下
![alt text](assets/0e77a7f438ca3f0d3b9973164ccb4af.jpg)
![alt text](assets/02eb60e4ac30aa7ce9f2cce9ca9a274.jpg)
![alt text](assets/de1b950c4aac8622dbb372f9aaf3dd5.jpg)

24.隐含马尔科夫模型（HMM）:常用的**序列概率密度描述模型**
25.在隐含马尔科夫模型中，**状态不可见，可见的是输出观察值**
![alt text](assets/2e84da7007c6b468219b504db6d918e.jpg)
![alt text](assets/f4ee86b0967636808ee6911b5b7b889.jpg)
![alt text](assets/83fd5505c6c975a2cf5f371864fcc63.jpg)
![alt text](assets/c6b2f3a3b8675e307bb4ecec62d801b.jpg)
![alt text](assets/2d45f5482f20e81c8ac3efe9a98f463.jpg)
![alt text](assets/014f41351a7c9a03ee71049ad80761b.jpg)
![alt text](assets/6a4eaf7d5a85ebf894ec4923a433712.jpg)
26.隐含马尔科夫模型的3个基本问题：估值问题、解码问题、学习问题